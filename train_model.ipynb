{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Introduction\n",
    "This model is an implementation of the FastRCNNPredictor Object detection model for the tensorflow Kaggle competition.\n",
    "\n",
    "This model is done using Pytorch and is interfaced using Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "# from torchvision import transforms\n",
    "from torch.utils import data\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "enable_gpu = True\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if enable_gpu:\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the Tensorboard Environment for monitoring the preformance of the model and the output states\n",
    "To open it run the following command\n",
    "\n",
    "``` bash\n",
    "  tensorboard --log_dir runs \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset shown below contains the metadata for each image and the annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>video_frame</th>\n",
       "      <th>sequence_frame</th>\n",
       "      <th>image_id</th>\n",
       "      <th>annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0-0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0-1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0-2</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0-3</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0-4</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23496</th>\n",
       "      <td>2</td>\n",
       "      <td>29859</td>\n",
       "      <td>10755</td>\n",
       "      <td>2983</td>\n",
       "      <td>2-10755</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23497</th>\n",
       "      <td>2</td>\n",
       "      <td>29859</td>\n",
       "      <td>10756</td>\n",
       "      <td>2984</td>\n",
       "      <td>2-10756</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23498</th>\n",
       "      <td>2</td>\n",
       "      <td>29859</td>\n",
       "      <td>10757</td>\n",
       "      <td>2985</td>\n",
       "      <td>2-10757</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23499</th>\n",
       "      <td>2</td>\n",
       "      <td>29859</td>\n",
       "      <td>10758</td>\n",
       "      <td>2986</td>\n",
       "      <td>2-10758</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23500</th>\n",
       "      <td>2</td>\n",
       "      <td>29859</td>\n",
       "      <td>10759</td>\n",
       "      <td>2987</td>\n",
       "      <td>2-10759</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23501 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       video_id  sequence  video_frame  sequence_frame image_id annotations\n",
       "0             0     40258            0               0      0-0          []\n",
       "1             0     40258            1               1      0-1          []\n",
       "2             0     40258            2               2      0-2          []\n",
       "3             0     40258            3               3      0-3          []\n",
       "4             0     40258            4               4      0-4          []\n",
       "...         ...       ...          ...             ...      ...         ...\n",
       "23496         2     29859        10755            2983  2-10755          []\n",
       "23497         2     29859        10756            2984  2-10756          []\n",
       "23498         2     29859        10757            2985  2-10757          []\n",
       "23499         2     29859        10758            2986  2-10758          []\n",
       "23500         2     29859        10759            2987  2-10759          []\n",
       "\n",
       "[23501 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Dataset\n",
    "\n",
    "These models are not designed to be trained on empty annotation sets. To filter these out and create a dataset that is useable, the annotation lists that are not empty are taken out.\n",
    "This does tamper with the index so the index is reset for easy indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>video_frame</th>\n",
       "      <th>sequence_frame</th>\n",
       "      <th>image_id</th>\n",
       "      <th>annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0-16</td>\n",
       "      <td>[{'x': 559, 'y': 213, 'width': 50, 'height': 32}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>0-17</td>\n",
       "      <td>[{'x': 558, 'y': 213, 'width': 50, 'height': 32}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0-18</td>\n",
       "      <td>[{'x': 557, 'y': 213, 'width': 50, 'height': 32}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0-19</td>\n",
       "      <td>[{'x': 556, 'y': 214, 'width': 50, 'height': 32}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0-20</td>\n",
       "      <td>[{'x': 555, 'y': 214, 'width': 50, 'height': 32}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4914</th>\n",
       "      <td>2</td>\n",
       "      <td>29859</td>\n",
       "      <td>10628</td>\n",
       "      <td>2856</td>\n",
       "      <td>2-10628</td>\n",
       "      <td>[{'x': 92, 'y': 532, 'width': 40, 'height': 37}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4915</th>\n",
       "      <td>2</td>\n",
       "      <td>29859</td>\n",
       "      <td>10629</td>\n",
       "      <td>2857</td>\n",
       "      <td>2-10629</td>\n",
       "      <td>[{'x': 78, 'y': 569, 'width': 40, 'height': 37}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4916</th>\n",
       "      <td>2</td>\n",
       "      <td>29859</td>\n",
       "      <td>10630</td>\n",
       "      <td>2858</td>\n",
       "      <td>2-10630</td>\n",
       "      <td>[{'x': 65, 'y': 606, 'width': 41, 'height': 37}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4917</th>\n",
       "      <td>2</td>\n",
       "      <td>29859</td>\n",
       "      <td>10631</td>\n",
       "      <td>2859</td>\n",
       "      <td>2-10631</td>\n",
       "      <td>[{'x': 51, 'y': 643, 'width': 44, 'height': 37}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4918</th>\n",
       "      <td>2</td>\n",
       "      <td>29859</td>\n",
       "      <td>10632</td>\n",
       "      <td>2860</td>\n",
       "      <td>2-10632</td>\n",
       "      <td>[{'x': 38, 'y': 681, 'width': 46, 'height': 37}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4919 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id  sequence  video_frame  sequence_frame image_id  \\\n",
       "0            0     40258           16              16     0-16   \n",
       "1            0     40258           17              17     0-17   \n",
       "2            0     40258           18              18     0-18   \n",
       "3            0     40258           19              19     0-19   \n",
       "4            0     40258           20              20     0-20   \n",
       "...        ...       ...          ...             ...      ...   \n",
       "4914         2     29859        10628            2856  2-10628   \n",
       "4915         2     29859        10629            2857  2-10629   \n",
       "4916         2     29859        10630            2858  2-10630   \n",
       "4917         2     29859        10631            2859  2-10631   \n",
       "4918         2     29859        10632            2860  2-10632   \n",
       "\n",
       "                                            annotations  \n",
       "0     [{'x': 559, 'y': 213, 'width': 50, 'height': 32}]  \n",
       "1     [{'x': 558, 'y': 213, 'width': 50, 'height': 32}]  \n",
       "2     [{'x': 557, 'y': 213, 'width': 50, 'height': 32}]  \n",
       "3     [{'x': 556, 'y': 214, 'width': 50, 'height': 32}]  \n",
       "4     [{'x': 555, 'y': 214, 'width': 50, 'height': 32}]  \n",
       "...                                                 ...  \n",
       "4914   [{'x': 92, 'y': 532, 'width': 40, 'height': 37}]  \n",
       "4915   [{'x': 78, 'y': 569, 'width': 40, 'height': 37}]  \n",
       "4916   [{'x': 65, 'y': 606, 'width': 41, 'height': 37}]  \n",
       "4917   [{'x': 51, 'y': 643, 'width': 44, 'height': 37}]  \n",
       "4918   [{'x': 38, 'y': 681, 'width': 46, 'height': 37}]  \n",
       "\n",
       "[4919 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno = train_data[train_data.annotations!='[]'].reset_index(drop=True)\n",
    "anno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that is done, we need to create our three dataset from this dataset in order to check the overall performance of the model. \n",
    "To do this, the dataset is converted to a numpy array and split using train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_index = anno.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([numpy_index[:,0],numpy_index[:,2]]).transpose()\n",
    "Y = numpy_index[:,-1]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.30)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Dataset\n",
    "\n",
    "This is the Pytorch Dataset for the which is then used for intergrating Pytorch's dataloader\n",
    "This class is made with the variblity needed for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, data_x, data_y, transforms = None):\n",
    "        self.images = data_x\n",
    "        self.anntations = data_y\n",
    "        self.transforms = transforms\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        video_num = self.images[index][0]\n",
    "        frame_num = self.images[index][1]\n",
    "        img = Image.open(f\"./train_images/video_{video_num}/{frame_num}.jpg\")\n",
    "        annotations_input = list(eval(self.anntations[index]))\n",
    "        annotation_list = []\n",
    "        area_list = []\n",
    "        for annotation in annotations_input:\n",
    "            xmin = annotation['x']\n",
    "            ymin = annotation['y']\n",
    "            xmax = xmin + annotation['width']\n",
    "            ymax = ymin + annotation['height']\n",
    "            area_list.append((ymax - ymin)*(xmax - xmin))\n",
    "            annotation_list.append([xmin, ymin, xmax, ymax])\n",
    "        \n",
    "        boxes = torch.as_tensor(annotation_list, dtype=torch.float16)\n",
    "        labels = torch.ones((len(annotation_list), ), dtype=torch.int64)\n",
    "        img_id = torch.tensor([frame_num])\n",
    "        areas = torch.as_tensor(area_list, dtype=torch.float32)\n",
    "        iscrowd = torch.zeros((len(annotation_list)), dtype=torch.int32)\n",
    "\n",
    "        annotations = {}\n",
    "        annotations['boxes'] = boxes\n",
    "        annotations['labels'] = labels\n",
    "        annotations['image_id'] = img_id\n",
    "        annotations['area'] = areas\n",
    "        annotations['iscrowd'] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        return img, annotations\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a minimal list of transforms.\n",
    "\n",
    "THe batch size is four at max due to memory limitations ( 6GB of VRAM is not enough for this model)\n",
    "The debug size is used when debugging the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_list = [ \n",
    "                  torchvision.transforms.ToTensor()]\n",
    "transforms = torchvision.transforms.Compose(transform_list)\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "if DEBUG:\n",
    "  batch_size = 2 #\n",
    "\n",
    "train_dataset = VisionDataset(x_train, y_train, transforms)\n",
    "\n",
    "val_dataset = VisionDataset(x_val, y_val, transforms)\n",
    "\n",
    "test_dataset = VisionDataset(x_test, y_test, transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Dataloaders\n",
    "\n",
    "Using the datasets above we create a dataloaders using Pytorch's built in dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "  return tuple(zip(*batch))\n",
    "\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=6, collate_fn=collate_fn)\n",
    "val_dataloader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=6, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "THe Resnet50 Model is the one in Pytorch's Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "\n",
    "  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "\n",
    "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Functions\n",
    "\n",
    "These functions consolidates the train and validate code into a single function inside each training loop\n",
    "\n",
    "The debug versions were made in order to find errors with the each step for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader : data.DataLoader, tenosrboard_writer : SummaryWriter, max_iters = None):\n",
    "  i = 0\n",
    "  losses = 0\n",
    "  for imgs, annotations in dataloader:\n",
    "    i += 1\n",
    "    imgs = list(img.to(device) for img in imgs)\n",
    "    annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "    loss_dict = model(imgs, annotations)\n",
    "    losses = sum(loss for loss in loss_dict.values())\n",
    "    optimizer.zero_grad()\n",
    "    losses.backward()\n",
    "    optimizer.step()\n",
    "    print(\"\\r\", end=\"\")\n",
    "    print(f'Iteration: {i}/{len(train_dataloader)}, Loss: {losses}', end=\"\")\n",
    "    tenosrboard_writer.add_scalar(\"Training Batch Losses\", losses)\n",
    "    if max_iters is not None:\n",
    "      if i > max_iters:\n",
    "        print(\"Exiting Validation Loop\")\n",
    "        break\n",
    "  \n",
    "  return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_debug(model, dataloader : data.DataLoader, tenosrboard_writer : SummaryWriter,  num_iters = 99999999):\n",
    "  i = 0\n",
    "  losses = 0\n",
    "  for imgs, annotations in dataloader:\n",
    "    i += 1\n",
    "    imgs = list(img.to(device) for img in imgs)\n",
    "    annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "    loss_dict = model(imgs, annotations)\n",
    "    losses = sum(loss for loss in loss_dict.values())\n",
    "    optimizer.zero_grad()\n",
    "    losses.backward()\n",
    "    optimizer.step()\n",
    "    print(\"\\r\", end=\"\")\n",
    "    print(f'Iteration: {i}/{len(train_dataloader)}, Loss: {losses}', end=\"\")\n",
    "    tenosrboard_writer.add_scalar(\"Training Batch Losses\", losses)\n",
    "    if i > num_iters:\n",
    "      print(\"Exiting Training Loop\")\n",
    "      break\n",
    "  \n",
    "  return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Functions\n",
    "\n",
    "These functions check the current accuracy of the model and publish it to tensorboard.\n",
    "It also take the first validation batch and checks puts those images onto the tensorboard to see how the model is progressing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader : data.DataLoader, tenosrboard_writer : SummaryWriter, img_count = 0, max_iters = None):\n",
    "  val_loss = 0\n",
    "  with torch.no_grad():\n",
    "    j = 0\n",
    "    i = 0\n",
    "    model.train()\n",
    "    for imgs, annotations in dataloader:\n",
    "      i += 1\n",
    "      imgs = list(img.to(device) for img in imgs)\n",
    "      annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "      loss_dict = model(imgs, annotations)\n",
    "      losses = sum(loss for loss in loss_dict.values())\n",
    "      if j == 0:\n",
    "        j = 1\n",
    "        model.eval()\n",
    "        pre_annotations = model(imgs)\n",
    "        imgs = list(img.to(\"cpu\") for img in imgs)\n",
    "        act_annotations = [{k: v.to(\"cpu\") for k, v in t.items()} for t in annotations]\n",
    "        act_imgs = []\n",
    "        pre_imgs = []\n",
    "        for i in range(len(imgs)):\n",
    "          act_imgs.append(torchvision.utils.draw_bounding_boxes(imgs[i].type(torch.uint8),\n",
    "                                                                act_annotations[i]['boxes']))\n",
    "          pre_imgs.append(torchvision.utils.draw_bounding_boxes(imgs[i].type(torch.uint8), \n",
    "                                                                pre_annotations[i]['boxes']))\n",
    "        act_grid = torchvision.utils.make_grid(act_imgs)\n",
    "        pre_grid = torchvision.utils.make_grid(pre_imgs)\n",
    "        tenosrboard_writer.add_image(\"Actual Labels\", act_grid, img_count)\n",
    "        tenosrboard_writer.add_image(\"Predicted Labels\", pre_grid, img_count)\n",
    "        model.train()\n",
    "        if max_iters is not None:\n",
    "          if i > max_iters:\n",
    "            print(\"Exiting Validation Loop\")\n",
    "            break\n",
    "        \n",
    "  return val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_debug(model, dataloader : data.DataLoader, tenosrboard_writer : SummaryWriter, img_count = 0, ):\n",
    "  val_loss = 0\n",
    "  with torch.no_grad():\n",
    "    j = 0\n",
    "    i = 0\n",
    "    model.train()\n",
    "    for imgs, annotations in dataloader:\n",
    "      i += 1\n",
    "      imgs = list(img.to(device) for img in imgs)\n",
    "      annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "      loss_dict = model(imgs, annotations)\n",
    "      val_loss = sum(loss for loss in loss_dict.values())\n",
    "      if j == 0:\n",
    "        model.eval()\n",
    "        j = 1\n",
    "        pre_annotations = model(imgs)\n",
    "        imgs = list(img.to(\"cpu\")*255 for img in imgs)\n",
    "        act_annotations = [{k: v.to(\"cpu\") for k, v in t.items()} for t in annotations]\n",
    "        act_imgs = []\n",
    "        pre_imgs = []\n",
    "        for i in range(len(imgs)):\n",
    "          act_imgs.append(torchvision.utils.draw_bounding_boxes(imgs[i].type(torch.uint8),\n",
    "                                                                act_annotations[i]['boxes']))\n",
    "          pre_imgs.append(torchvision.utils.draw_bounding_boxes(imgs[i].type(torch.uint8), \n",
    "                                                                pre_annotations[i]['boxes']))\n",
    "        act_grid = torchvision.utils.make_grid(act_imgs)\n",
    "        pre_grid = torchvision.utils.make_grid(pre_imgs)\n",
    "        tenosrboard_writer.add_image(\"Actual Labels\", act_grid, img_count)\n",
    "        tenosrboard_writer.add_image(\"Predicted Labels\", pre_grid, img_count)\n",
    "        model.train()\n",
    "        \n",
    "      if i > max_iters:\n",
    "        print(\"Exiting Validation Loop\")\n",
    "        break\n",
    "  return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters (Sans Batch Size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "num_epochs = 40\n",
    "if DEBUG:\n",
    "  num_epochs = 2\n",
    "\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr = 0.005, momentum=0.9, weight_decay=0.005)\n",
    "losses = 0\n",
    "val_loss = 0\n",
    "num_batches_run = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 431/431, Loss: 0.23095811903476715Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.49440270662307743Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.18438373506069183Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.22802816331386566Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.16689980030059814Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.17525008320808413Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.36067140102386475Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.44644123315811167Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.30061116814613343Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.29209059476852417Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.45992541313171387Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.28977116942405739Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.32302826642990114Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.22836530208587646Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.16671857237815857Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.55061763525009165Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.12441346049308777Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.13273245096206665Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.19217190146446228Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.14332573115825653Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.42714294791221626Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.14454182982444763Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.33819803595542917Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.22931036353111267Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.11988186091184616Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.15943890810012817Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.39242613315582275Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.28690913319587718Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.14047174155712128Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.14072106778621674Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.16051736474037175Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.24488097429275513Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.21309520304203033Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.12684592604637146Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.12678289413452148Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.20997066795825958Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.21382631361484528Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.14627525210380554Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.37220194935798645Epoch Length: 431, Train Loss: 0, Val Loss: 0\n",
      "Iteration: 431/431, Loss: 0.38821399211883545Epoch Length: 431, Train Loss: 0, Val Loss: 0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "  train_losses = 0\n",
    "  val_loss = 0\n",
    "  if DEBUG:\n",
    "    train_losses = train_debug(model, train_dataloader, writer, 10)\n",
    "    val_loss = validate_debug(model, val_dataloader, writer, epoch, 10)\n",
    "  else:\n",
    "    train_losses = train(model, train_dataloader, writer)\n",
    "    val_loss = validate(model, val_dataloader, writer, epoch)\n",
    "  print(f'Epoch Length: {len(train_dataloader)}, Train Loss: {losses}, Val Loss: {val_loss}')\n",
    "  writer.add_scalars(\"Epoch Losses\", {\"Test\":train_losses, \"Validation\":val_loss}, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Losses: 0.2644938826560974\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = data.DataLoader(test_dataset, 1, num_workers=12, collate_fn=collate_fn)\n",
    "\n",
    "test_loss_values = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "model.train()\n",
    "\n",
    "with torch.no_grad():\n",
    "  for imgs, annotations in test_dataloader:\n",
    "    imgs = list(img.to(device) for img in imgs)\n",
    "    annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "    loss_dict = model(imgs, annotations)\n",
    "    test_loss = sum(loss for loss in loss_dict.values())\n",
    "    test_loss_values.append(test_loss)\n",
    "\n",
    "print(f'Test Losses: {sum(test_loss_values)/len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"Model_Result.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5bfc880fb74eeeefc0ac833e27cc819b72d6a19dd59473779f2dde26045d8f2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
